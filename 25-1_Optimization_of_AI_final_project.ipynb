{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4q4776_H_6r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass, field\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "# ─── 1. 설정 클래스 (기존 config.py) ──────────────────────────────────\n",
        "@dataclass\n",
        "class Config:\n",
        "    # 데이터 관련\n",
        "    csv_path: str = 'berkshire_lstm.csv'\n",
        "    feature_cols: list[str] = field(default_factory=lambda: ['Close', 'Volume', 'Measure'])\n",
        "    target_col: str = 'Close'\n",
        "    \n",
        "    # 모델 하이퍼파라미터 (GA 최적화 대상)\n",
        "    seq_len: int = 60\n",
        "    hidden_size: int = 64\n",
        "    num_layers: int = 2\n",
        "    dropout: float = 0.2\n",
        "    lr: float = 1e-3\n",
        "    batch_size: int = 128\n",
        "\n",
        "    # 고정 학습 파라미터\n",
        "    train_ratio: float = 0.7\n",
        "    val_ratio: float = 0.15\n",
        "    num_epochs: int = 50  # GA 실행 시간을 고려해 에폭 수 조정\n",
        "    patience: int = 5\n",
        "\n",
        "    # 실행 환경\n",
        "    device: torch.device = field(default_factory=lambda: \n",
        "                                  torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.input_size = len(self.feature_cols)\n",
        "\n",
        "# ─── 2. 데이터 처리 (기존 data.py) ───────────────────────────────────────\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float().unsqueeze(-1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, parse_dates=['Date'])\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df.sort_index()\n",
        "\n",
        "def split_df(df: pd.DataFrame, cfg: Config):\n",
        "    n = len(df)\n",
        "    n_train = int(n * cfg.train_ratio)\n",
        "    n_val = int(n * (cfg.train_ratio + cfg.val_ratio))\n",
        "    train_df = df.iloc[:n_train]\n",
        "    val_df = df.iloc[n_train:n_val]\n",
        "    test_df = df.iloc[n_val:]\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def create_sequences(features: np.ndarray, target: np.ndarray, seq_len: int):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - seq_len):\n",
        "        X.append(features[i : i + seq_len])\n",
        "        y.append(target[i + seq_len])\n",
        "    return np.stack(X), np.array(y)\n",
        "\n",
        "def make_datasets(cfg: Config):\n",
        "    df = load_data(cfg.csv_path)\n",
        "    tr_df, va_df, te_df = split_df(df, cfg)\n",
        "    \n",
        "    feat_scaler = StandardScaler().fit(tr_df[cfg.feature_cols])\n",
        "    targ_scaler = StandardScaler().fit(tr_df[[cfg.target_col]])\n",
        "    \n",
        "    tr_feats = feat_scaler.transform(tr_df[cfg.feature_cols])\n",
        "    tr_targ = targ_scaler.transform(tr_df[[cfg.target_col]]).flatten()\n",
        "    va_feats = feat_scaler.transform(va_df[cfg.feature_cols])\n",
        "    va_targ = targ_scaler.transform(va_df[[cfg.target_col]]).flatten()\n",
        "    te_feats = feat_scaler.transform(te_df[cfg.feature_cols])\n",
        "    te_targ = targ_scaler.transform(te_df[[cfg.target_col]]).flatten()\n",
        "\n",
        "    X_tr, y_tr = create_sequences(tr_feats, tr_targ, cfg.seq_len)\n",
        "    X_va, y_va = create_sequences(va_feats, va_targ, cfg.seq_len)\n",
        "    X_te, y_te = create_sequences(te_feats, te_targ, cfg.seq_len)\n",
        "\n",
        "    train_ds = TimeSeriesDataset(X_tr, y_tr)\n",
        "    valid_ds = TimeSeriesDataset(X_va, y_va)\n",
        "    test_ds = TimeSeriesDataset(X_te, y_te)\n",
        "    \n",
        "    return train_ds, valid_ds, test_ds, feat_scaler, targ_scaler\n",
        "\n",
        "# ─── 3. 모델 정의 (기존 model.py) ─────────────────────────────────────────\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        last_hidden = out[:, -1, :]\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "def build_model(cfg: Config) -> nn.Module:\n",
        "    model = LSTMRegressor(\n",
        "        input_size=cfg.input_size,\n",
        "        hidden_size=cfg.hidden_size,\n",
        "        num_layers=cfg.num_layers,\n",
        "        dropout=cfg.dropout\n",
        "    )\n",
        "    return model.to(cfg.device)\n",
        "\n",
        "# ─── 4. 학습 및 평가 함수 (기존 train.py, evaluate.py) ────────────────\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * X.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "def train_model(cfg: Config, train_ds: Dataset, valid_ds: Dataset) -> tuple[nn.Module, float]:\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "    \n",
        "    model = build_model(cfg).to(cfg.device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(1, cfg.num_epochs + 1):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, cfg.device)\n",
        "        val_loss = validate_epoch(model, valid_loader, criterion, cfg.device)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_model_state = deepcopy(model.state_dict())\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= cfg.patience:\n",
        "            # print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "            \n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, best_val_loss\n",
        "\n",
        "def evaluate_model(model: nn.Module, test_ds: Dataset, targ_scaler: StandardScaler, cfg: Config):\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "    \n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(cfg.device), y.to(cfg.device)\n",
        "            preds.append(model(X).cpu().numpy())\n",
        "            trues.append(y.cpu().numpy())\n",
        "            \n",
        "    preds = np.vstack(preds).flatten()\n",
        "    trues = np.vstack(trues).flatten()\n",
        "\n",
        "    preds_inv = targ_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
        "    trues_inv = targ_scaler.inverse_transform(trues.reshape(-1, 1)).flatten()\n",
        "\n",
        "    mae = mean_absolute_error(trues_inv, preds_inv)\n",
        "    rmse = np.sqrt(mean_squared_error(trues_inv, preds_inv))\n",
        "    mape = np.mean(np.abs((trues_inv - preds_inv) / (trues_inv + 1e-8))) * 100\n",
        "\n",
        "    print(\"\\n--- 최종 모델 평가 결과 ---\")\n",
        "    print(f\"Test MAE: {mae:.4f}\")\n",
        "    print(f\"Test RMSE: {rmse:.4f}\")\n",
        "    print(f\"Test MAPE: {mape:.2f}%\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(trues_inv, label='Actual')\n",
        "    plt.plot(preds_inv, label='Predicted', linestyle='--')\n",
        "    plt.title('Final Model: Actual vs. Predicted')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# ─── 5. 유전 알고리즘 (GA) ───────────────────────────────────────────────\n",
        "# 하이퍼파라미터 탐색 공간\n",
        "HPARAM_SPACE = {\n",
        "    'hidden_size': [32, 64, 128],\n",
        "    'num_layers': [1, 2, 3],\n",
        "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
        "    'lr': [1e-4, 5e-4, 1e-3, 5e-3],\n",
        "    'seq_len': [30, 60, 90, 120],\n",
        "    'batch_size': [32, 64, 128]\n",
        "}\n",
        "\n",
        "# GA 설정\n",
        "POPULATION_SIZE = 10\n",
        "NUM_GENERATIONS = 5\n",
        "CX_PROB = 0.7  # 교차 확률\n",
        "MUT_PROB = 0.3 # 변이 확률\n",
        "\n",
        "def create_individual() -> dict:\n",
        "    \"\"\"랜덤 하이퍼파라미터 조합(개체) 생성\"\"\"\n",
        "    return {k: random.choice(v) for k, v in HPARAM_SPACE.items()}\n",
        "\n",
        "def evaluate_fitness(individual: dict, base_cfg: Config, verbose=True) -> float:\n",
        "    \"\"\"\n",
        "    개체(하이퍼파라미터 셋)의 적합도(검증 손실) 평가\n",
        "    \"\"\"\n",
        "    # 개별 설정을 담을 Config 객체 생성\n",
        "    cfg = deepcopy(base_cfg)\n",
        "    for key, value in individual.items():\n",
        "        setattr(cfg, key, value)\n",
        "    \n",
        "    # 데이터셋 생성 (seq_len이 바뀔 수 있으므로 매번 생성)\n",
        "    try:\n",
        "        train_ds, valid_ds, _, _, _ = make_datasets(cfg)\n",
        "    except Exception as e:\n",
        "        print(f\"데이터셋 생성 중 오류 (Hparams: {individual}): {e}\")\n",
        "        return float('inf') # 에러 발생 시 최악의 점수 반환\n",
        "\n",
        "    if len(train_ds) == 0 or len(valid_ds) == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # 모델 학습 및 검증\n",
        "    _, val_loss = train_model(cfg, train_ds, valid_ds)\n",
        "    \n",
        "    if verbose:\n",
        "        param_str = \", \".join([f\"{k}:{v}\" for k,v in individual.items()])\n",
        "        print(f\"  HParams: [{param_str}] -> Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "    return val_loss\n",
        "\n",
        "def tournament_selection(population: list, fitnesses: list, k=3) -> dict:\n",
        "    \"\"\"토너먼트 선택\"\"\"\n",
        "    selection_ix = np.random.randint(len(population))\n",
        "    for ix in np.random.randint(0, len(population), k - 1):\n",
        "        if fitnesses[ix] < fitnesses[selection_ix]:\n",
        "            selection_ix = ix\n",
        "    return population[selection_ix]\n",
        "\n",
        "def crossover(p1: dict, p2: dict) -> tuple[dict, dict]:\n",
        "    \"\"\"단일점 교차\"\"\"\n",
        "    c1, c2 = p1.copy(), p2.copy()\n",
        "    if random.random() < CX_PROB:\n",
        "        keys = list(HPARAM_SPACE.keys())\n",
        "        pt = random.randint(1, len(keys) - 1)\n",
        "        for i in range(pt, len(keys)):\n",
        "            key = keys[i]\n",
        "            c1[key], c2[key] = c2[key], c1[key]\n",
        "    return c1, c2\n",
        "\n",
        "def mutate(individual: dict) -> dict:\n",
        "    \"\"\"랜덤 변이\"\"\"\n",
        "    if random.random() < MUT_PROB:\n",
        "        key_to_mutate = random.choice(list(HPARAM_SPACE.keys()))\n",
        "        current_value = individual[key_to_mutate]\n",
        "        possible_values = [v for v in HPARAM_SPACE[key_to_mutate] if v != current_value]\n",
        "        if possible_values:\n",
        "            individual[key_to_mutate] = random.choice(possible_values)\n",
        "    return individual\n",
        "\n",
        "# ─── 6. 메인 실행 로직 ───────────────────────────────────────────────────\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 기본 설정 로드\n",
        "    base_cfg = Config()\n",
        "    \n",
        "    # --- 유전 알고리즘 시작 ---\n",
        "    print(\"--- Genetic Algorithm for Hyperparameter Optimization ---\")\n",
        "    \n",
        "    # 1. 초기 집단 생성\n",
        "    population = [create_individual() for _ in range(POPULATION_SIZE)]\n",
        "    best_individual = None\n",
        "    best_fitness = float('inf')\n",
        "\n",
        "    for gen in range(1, NUM_GENERATIONS + 1):\n",
        "        print(f\"\\n[Generation {gen}/{NUM_GENERATIONS}]\")\n",
        "        \n",
        "        # 2. 적합도 평가\n",
        "        fitnesses = [evaluate_fitness(ind, base_cfg) for ind in population]\n",
        "\n",
        "        # 현재 세대 최고 기록 갱신\n",
        "        for i in range(POPULATION_SIZE):\n",
        "            if fitnesses[i] < best_fitness:\n",
        "                best_fitness = fitnesses[i]\n",
        "                best_individual = population[i]\n",
        "                print(f\"  >> New Best Found! Val Loss: {best_fitness:.6f}\")\n",
        "\n",
        "        # 3. 다음 세대 생성\n",
        "        selected = [tournament_selection(population, fitnesses) for _ in range(POPULATION_SIZE)]\n",
        "        next_population = []\n",
        "        for i in range(0, POPULATION_SIZE, 2):\n",
        "            p1, p2 = selected[i], selected[i+1]\n",
        "            c1, c2 = crossover(p1, p2)\n",
        "            next_population.append(mutate(c1))\n",
        "            next_population.append(mutate(c2))\n",
        "        population = next_population\n",
        "\n",
        "    print(\"\\n--- GA Optimization Finished ---\")\n",
        "    print(f\"Total time: {(time.time() - start_time)/60:.2f} minutes\")\n",
        "    print(f\"Best validation loss: {best_fitness:.6f}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for key, val in best_individual.items():\n",
        "        print(f\"  - {key}: {val}\")\n",
        "\n",
        "    # --- 최적 하이퍼파라미터로 최종 학습 및 평가 ---\n",
        "    print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
        "    final_cfg = deepcopy(base_cfg)\n",
        "    for key, value in best_individual.items():\n",
        "        setattr(final_cfg, key, value)\n",
        "    \n",
        "    # 데이터 다시 로드\n",
        "    train_ds, valid_ds, test_ds, _, targ_scaler = make_datasets(final_cfg)\n",
        "    \n",
        "    # 최종 모델 학습 (더 긴 에폭으로 학습 가능)\n",
        "    final_cfg.num_epochs = 100 \n",
        "    final_cfg.patience = 10\n",
        "    print(f\"Final training with config: {final_cfg}\")\n",
        "    final_model, _ = train_model(final_cfg, train_ds, valid_ds)\n",
        "    \n",
        "    # 체크포인트 저장\n",
        "    save_dir = 'checkpoints'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, 'best_ga_model.pth')\n",
        "    torch.save(final_model.state_dict(), save_path)\n",
        "    print(f\"Final model saved to {save_path}\")\n",
        "\n",
        "    # 최종 모델 평가\n",
        "    evaluate_model(final_model, test_ds, targ_scaler, final_cfg)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 실행 전, CSV 파일 경로를 확인해주세요.\n",
        "    # 예: cfg = Config(csv_path='data/my_stock_data.csv')\n",
        "    # 아래 data 폴더와 파일을 생성해야 오류 없이 실행됩니다.\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    \n",
        "    # 샘플 데이터 파일 생성\n",
        "    if not os.path.exists('data/your_data.csv'):\n",
        "        print(\"Creating dummy data file 'data/your_data.csv'...\")\n",
        "        sample_dates = pd.date_range(start='2020-01-01', periods=500)\n",
        "        sample_data = pd.DataFrame({\n",
        "            'Date': sample_dates,\n",
        "            'Close': np.random.rand(500).cumsum() + 50,\n",
        "            'Volume': np.random.randint(1000, 5000, 500),\n",
        "            'Measure': np.random.randn(500).cumsum()\n",
        "        })\n",
        "        sample_data.to_csv('data/your_data.csv', index=False)\n",
        "        \n",
        "    main() "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
